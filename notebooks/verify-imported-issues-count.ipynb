{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare number of local issues against issues on s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason for this notebook is: https://github.com/impresso/impresso-data-sanitycheck/issues/9\n",
    "\n",
    "The `venv` from which this notebooks runs gives you access to the following libs:\n",
    "- https://github.com/impresso/impresso-data-sanitycheck\n",
    "- impresso-pycommons\n",
    "- impresso-text-importer\n",
    "\n",
    "Code is reused as as much as possible from the respective importers and other submodules `sanity_check.contents.s3_data`, `sanity_check.contents.checks`, `sanity_check.contents.stats` and `sanity_check.contents.sync` also contain quite a bit of code to load s3 data into dataframes for various purposes, see e.g.  https://github.com/impresso/impresso-data-sanitycheck/blob/5c47b1d8360570f749909d07e64c0289057c243f/sanity_check/contents/stats.py#L266-L286."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romanell/.pyenv/versions/3.7.4/envs/impresso-sanity-check/lib/python3.7/site-packages/python_jsonschema_objects/__init__.py:53: UserWarning: Schema version http://json-schema.org/draft-06/schema# not recognized. Some keywords and features may not be supported.\n",
      "  self.schema[\"$schema\"]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "from impresso_commons.path.path_fs import detect_issues\n",
    "from text_importer.importers.rero.detect import detect_issues as rero_detect_issues\n",
    "from text_importer.importers.lux.detect import detect_issues as lux_detect_issues\n",
    "from text_importer.importers.bnf.detect import detect_issues as bnf_detect_issues\n",
    "from text_importer.importers.bnf_en.detect import detect_issues as bnfen_detect_issues, dir2issue, BnfEnIssueDir\n",
    "from text_importer.importers.bl.detect import detect_issues as bl_detect_issues\n",
    "from text_importer.importers.swa.detect import detect_issues as swa_detect_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "from dask import bag as db\n",
    "from dask import dataframe as dd\n",
    "from dask import array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sanity_check.contents.s3_data import fetch_issue_ids, fetch_issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine where this notebook runs has 48 cores and quite a bit of RAM, so you can make use of that if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:41089</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>48</li>\n",
       "  <li><b>Memory: </b>270.38 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:41089' processes=8 threads=48, memory=270.38 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the original (input) data for the canonical ingestion are:\n",
    "\n",
    "```\n",
    "/mnt/project_impresso/original/BNL\n",
    "/mnt/project_impresso/original/RERO\n",
    "/mnt/project_impresso/original/RERO2\n",
    "/mnt/project_impresso/original/RERO3\n",
    "/mnt/impresso_syno/01_GDL\n",
    "/mnt/impresso_syno/02_GDL\n",
    "/mnt/impresso_syno/01_JDG\n",
    "/mnt/impresso_syno/02_JDG\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it should be a script that takes in input a list of paths (pointing to EPFL's NAS where the raw OCR data is) plus an s3 bucket path (where the ingested canonical is);\n",
    "\n",
    "- to each base path the correct detect function is applied\n",
    "- resulting issues are then grouped to produce counts by newspaper/year\n",
    "- then we read canonical data from s3 and produce similar counts by newspaper/year (trivial because data is already packaged this way)\n",
    "- at the end we combine the two sets of counts, write it to e.g. CSV and only flag (print) cases where the difference is above a certain user-specified threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def bnfen_custom_detect_issues(base_dir: str, access_rights: str=None):\n",
    "    \"\"\"Detect newspaper issues to import within the filesystem.\n",
    "    This function expects the directory structure that BNF-EN used to\n",
    "    organize the dump of Mets/Alto OCR data.\n",
    "    :param str base_dir: Path to the base directory of newspaper data.\n",
    "    :param str access_rights: Not used for this imported, but argument is kept for normality\n",
    "    :return: List of `BnfEnIssueDir` instances, to be imported.\n",
    "    \"\"\"\n",
    "    \n",
    "    dir_path, dirs, files = next(os.walk(base_dir))\n",
    "    journal_dirs = [os.path.join(dir_path, _dir) for _dir in dirs]\n",
    "    issue_dirs = [\n",
    "        os.path.join(journal, _dir)\n",
    "        for journal in journal_dirs\n",
    "        for _dir in os.listdir(journal)\n",
    "        ]\n",
    "    \n",
    "    issue_dirs = [bnfen_dir2issue(_dir, None) for _dir in issue_dirs]\n",
    "    \n",
    "    issue_dirs = [i for i in issue_dirs if i is not None]\n",
    "    \n",
    "    return issue_dirs\n",
    "\n",
    "def bnfen_dir2issue(path: str, access_rights: dict):\n",
    "    \"\"\"Create a `BnfEnIssueDir` object from a directory path.\n",
    "    .. note ::\n",
    "        This function is called internally by :func:`detect_issues`\n",
    "    :param str path: Path of issue.\n",
    "    :return: New ``BnfEnIssueDir`` object\n",
    "    \"\"\"\n",
    "    journal, issue = path.split('/')[-2:]\n",
    "    \n",
    "    date, edition = issue.split('_')[:2]\n",
    "    date = datetime.strptime(date, '%Y%m%d').date()\n",
    "    journal = journal.lower().replace('-', '').strip()\n",
    "    edition = 'X'\n",
    "    \n",
    "\n",
    "    \n",
    "    return BnfEnIssueDir(journal=journal, date=date, edition=edition, path=path,\n",
    "                         rights=\"open-public\", ark_link=\"IIIF_LINK\")\n",
    "\n",
    "def detect_issues_from_dirs(local_dirs:list):\n",
    "    \"\"\"\n",
    "    Wrapper to detect issues for various sources and file structures\n",
    "    \"\"\"\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    \n",
    "    for path in local_dirs:\n",
    "        path_lower = path.lower()\n",
    "        if 'rero2' in path_lower:\n",
    "            issues += rero_detect_issues(path, \"/mnt/project_impresso/original/RERO2/rero2_access_rights.json\")\n",
    "        elif 'rero3' in path_lower:\n",
    "            issues += rero_detect_issues(path, \"/mnt/project_impresso/original/RERO3/access_rights.json\")\n",
    "        elif 'bnl' in path_lower:\n",
    "            issues += lux_detect_issues(path)\n",
    "        elif 'bnf-en' in path_lower:\n",
    "            issues += bnfen_custom_detect_issues(path)\n",
    "        elif 'bnf' in path_lower:\n",
    "            issues += bnf_detect_issues(path, \"/mnt/project_impresso/original/BNF/access_rights.json\")\n",
    "        elif 'bl' in path_lower:\n",
    "            issues += bl_detect_issues(path, access_rights=None, tmp_dir='tmp_bnl_uncompressed')\n",
    "        elif 'swa' in path_lower:\n",
    "            issues += swa_detect_issues(path, \"/mnt/project_impresso/original/SWA/access_rights.json\")\n",
    "        else:\n",
    "            issues += detect_issues(path)\n",
    "            \n",
    "\n",
    "    return issues \n",
    "\n",
    "\n",
    "def canonical_issue_meta_from_id(issue_id):\n",
    "    journal, year, month, day, edition = issue_id.split('-')\n",
    "    meta = {\"journal\": journal, \"year\": year, \"issue_id\": issue_id}\n",
    "    \n",
    "    return meta\n",
    "\n",
    "def canonical_issue_name(issues):\n",
    "    \"\"\"\n",
    "    Create a canonical issue id from an `IssueDir` object.\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = []\n",
    "    \n",
    "    for issue in issues:\n",
    "        issue_id = \"-\".join(\n",
    "                        [\n",
    "                            issue.journal,\n",
    "                            str(issue.date.year),\n",
    "                            str(issue.date.month).zfill(2),\n",
    "                            str(issue.date.day).zfill(2),\n",
    "                            issue.edition\n",
    "                        ]\n",
    "                    )\n",
    "        \n",
    "        ret.append([issue.journal, issue.date.year, issue_id])\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching list of newspapers from s3://canonical-data\n",
      "canonical-data contains 93 newspapers\n",
      "s3://canonical-data contains 3882 .bz2 files with issues\n",
      "Fetching issue ids from 3882 .bz2 files (compute=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal</th>\n",
       "      <th>year</th>\n",
       "      <th>n_issues_local</th>\n",
       "      <th>n_issues_s3</th>\n",
       "      <th>coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>EVT</td>\n",
       "      <td>2002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>EXP</td>\n",
       "      <td>1779</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>EXP</td>\n",
       "      <td>1823</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>EXP</td>\n",
       "      <td>1824</td>\n",
       "      <td>50.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>EXP</td>\n",
       "      <td>2005</td>\n",
       "      <td>303.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>0.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3790</th>\n",
       "      <td>oeuvre</td>\n",
       "      <td>1937</td>\n",
       "      <td>364.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>0.997253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3791</th>\n",
       "      <td>oeuvre</td>\n",
       "      <td>1938</td>\n",
       "      <td>363.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>0.997245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3792</th>\n",
       "      <td>oeuvre</td>\n",
       "      <td>1939</td>\n",
       "      <td>364.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>0.994505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>oeuvre</td>\n",
       "      <td>1940</td>\n",
       "      <td>336.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>0.997024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>oeuvre</td>\n",
       "      <td>1942</td>\n",
       "      <td>311.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.938907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     journal  year  n_issues_local  n_issues_s3  coverage\n",
       "127      EVT  2002             1.0          NaN       NaN\n",
       "140      EXP  1779             2.0          NaN       NaN\n",
       "167      EXP  1823           100.0         50.0  0.500000\n",
       "168      EXP  1824            50.0         49.0  0.980000\n",
       "349      EXP  2005           303.0        302.0  0.996700\n",
       "...      ...   ...             ...          ...       ...\n",
       "3790  oeuvre  1937           364.0        363.0  0.997253\n",
       "3791  oeuvre  1938           363.0        362.0  0.997245\n",
       "3792  oeuvre  1939           364.0        362.0  0.994505\n",
       "3793  oeuvre  1940           336.0        335.0  0.997024\n",
       "3795  oeuvre  1942           311.0        292.0  0.938907\n",
       "\n",
       "[614 rows x 5 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aggr_by_year_journal(df):\n",
    "    return df.groupby(['journal', 'year']) \\\n",
    "    .count()\n",
    "\n",
    "def filter_sources_with_mismatch(df, thres = 1.0):\n",
    "    df['coverage'] = None\n",
    "    df['coverage'] = df['n_issues_s3'] / df['n_issues_local']\n",
    "    df = df[(df.coverage < thres) | (df.coverage.isna())] \n",
    "    \n",
    "    return df\n",
    "\n",
    "def run_issue_comparison(s3_bucket:str, local_dirs:list, f_out_full:str=None, f_out_mismatch:str=None):\n",
    "\n",
    "    issues_local = db.from_sequence(local_dirs, partition_size=1) \\\n",
    "                    .map_partitions(detect_issues_from_dirs) \\\n",
    "                    .compute()\n",
    "\n",
    "    df_local = db.from_sequence(issues_local) \\\n",
    "                .map_partitions(canonical_issue_name) \\\n",
    "                .to_dataframe(meta={'journal': str, 'year': int, 'issue_id': str}) \\\n",
    "                .compute()\n",
    "\n",
    "\n",
    "    df_n_issues_local = aggr_by_year_journal(df_local) \\\n",
    "                            .rename(columns={\"issue_id\": \"n_issues_local\"})\n",
    "\n",
    "\n",
    "    s3_canonical_issue_ids = fetch_issue_ids(bucket_name=s3_bucket)\n",
    "\n",
    "    df_s3 = db.from_sequence(s3_canonical_issue_ids) \\\n",
    "        .map(canonical_issue_meta_from_id) \\\n",
    "        .to_dataframe(meta={'journal': str, 'year': int, 'issue_id': str}) \\\n",
    "        .compute()\n",
    "\n",
    "\n",
    "    df_n_issues_s3 = aggr_by_year_journal(df_s3) \\\n",
    "                            .rename(columns={\"issue_id\": \"n_issues_s3\"})\n",
    "\n",
    "    df_comb = df_n_issues_local.merge(df_n_issues_s3, how=\"outer\", left_index=True, right_index=True).reset_index()\n",
    "\n",
    "    df_err = filter_sources_with_mismatch(df_comb)\n",
    "    \n",
    "    if f_out_full:\n",
    "        df_comb.to_csv(f_out_full)\n",
    "    if f_out_mismatch:\n",
    "        df_err.to_csv(f_out_mismatch)\n",
    "\n",
    "    return df_err\n",
    "\n",
    "\n",
    "s3_bucket='s3://canonical-data'\n",
    "\n",
    "orig_resources = [\n",
    "    \"/mnt/project_impresso/original/BNL\",\n",
    "    \"/mnt/project_impresso/original/RERO\",\n",
    "    \"/mnt/project_impresso/original/RERO2\",\n",
    "    \"/mnt/project_impresso/original/RERO3\",\n",
    "    \"/mnt/impresso_syno\",\n",
    "    \"/mnt/project_impresso/original/BNF\",\n",
    "    \"/mnt/project_impresso/original/BNF-EN\",\n",
    "    \"/mnt/project_impresso/original/BL\",\n",
    "    \"/mnt/project_impresso/original/SWA\"  \n",
    "]\n",
    "    \n",
    "\n",
    "f_out_mismatch = 'data_ingestion_issue_mismatch.csv'\n",
    "f_out_full = 'data_ingestion_issue_overview.csv'\n",
    "\n",
    "df_err = run_issue_comparison(s3_bucket=s3_bucket, local_dirs=orig_resources, f_out_mismatch=f_out_mismatch, f_out_full=f_out_full)\n",
    "df_err"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "416.733px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
