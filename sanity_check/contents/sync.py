import pandas as pd
from .s3_data import fetch_issue_ids, fetch_issue_ids_rebuilt
from .mysql import list_issues as mysql_list_issues


def check_sync_db(s3_bucket_name: str, mysql_db_config: str) -> pd.DataFrame:
    """
    Check which canonical issues from S3 are not present in the DB.

    # TODO: serialise dataframe somewhere

    Return a dataframe with detailed information.
    """

    # get list of issue IDs from s3
    s3_issues_ids = fetch_issue_ids(s3_bucket_name)

    # do the same for MySQL
    mysql_issue_ids = mysql_list_issues(mysql_db_config)

    # create dataframe with issue ids from s3
    s3_issue_data = pd.DataFrame(
        [
            {
                "id": issue_id,
                "in_s3": True
            }
            for issue_id in s3_issues_ids
        ]
    ).set_index('id')
    # create dataframe with issue ids from MySQL
    mysql_issue_data = pd.DataFrame(
        [
            {
                "id": issue_id,
                "in_mysql": True
            }
            for issue_id in mysql_issue_ids
        ]
    ).set_index('id')

    # we combine the two sets of IDs with an outer join
    issue_data = mysql_issue_data.join(s3_issue_data, how='outer')

    # and return only those that are in S3 but not in MySQL
    issues_to_ingest = issue_data[
        (issue_data.in_mysql.isnull()) & issue_data.in_s3.notnull()
    ]
    issues_to_ingest['newspaper_id'] = issues_to_ingest.index.map(
        lambda i: i.split('-')[0]
    )
    issues_to_ingest['year'] = issues_to_ingest.index.map(
        lambda i: int(i.split('-')[1])
    )

    print((
        f'There are {issues_to_ingest.shape[0]} issues from {s3_bucket_name} '
        f'missing from MySQL ({mysql_db_config}).'
    ))

    return issues_to_ingest


def check_sync_rebuilt(
    canonical_bucket_name: str,
    rebuilt_bucket_name: str
) -> tuple:
    """
    Check which canonical issues have not been rebuilt, and which rebuilt
    data are not yet ingested into canonical.

    # TODO: serialise dataframes somewhere

    Return a dataframe with detailed information.
    """

    s3_canonical_issues = fetch_issue_ids(
        canonical_bucket_name,
        compute=False
    )
    s3_rebuilt_issues = fetch_issue_ids_rebuilt(
        rebuilt_bucket_name,
        compute=False
    )

    s3_rebuilt_data = s3_rebuilt_issues.map(
        lambda i: {'id': i, "in_rebuilt": True}
    ).to_dataframe().set_index('id').compute()

    s3_canonical_data = s3_canonical_issues.map(
        lambda i: {'id': i, "in_canonical": True}
    ).to_dataframe().set_index('id').compute()

    print('Joining the two dataframes')
    issue_data = s3_rebuilt_data.join(
        s3_canonical_data,
        how='outer'
    )
    print('Joining the two dataframes... done')

    issue_data['newspaper_id'] = issue_data.index.map(
        lambda i: i.split('-')[0]
    )
    issue_data['year'] = issue_data.index.map(
        lambda i: int(i.split('-')[1])
    )

    issues_to_rebuild = issue_data[
        (issue_data.in_canonical.notnull()) &
        (issue_data.in_rebuilt.isnull())
    ]
    issues_to_ingest = issue_data[
        (issue_data.in_canonical.isnull()) &
        (issue_data.in_rebuilt.notnull())
    ]

    return(issues_to_ingest, issues_to_rebuild)


def configure_db_ingestion(s3_bucket_name: str, mysql_db_config: str) -> list:
    """Generate the config file for DB ingestion in a data-driven fashion."""
    config = []

    issues_to_ingest = check_sync_db(s3_bucket_name, mysql_db_config)

    for key, group in issues_to_ingest.groupby(by='newspaper_id'):
        missing_years = sorted(set(group.year))
        for year in missing_years:
            config.append(
                {
                    key: [year, year + 1]
                }
            )
    return config


def configure_rebuild(
    canonical_bucket_name: str,
    rebuilt_bucket_name: str,
    issues_to_rebuild: pd.DataFrame = None
) -> list:
    """Generate the config file for data rebuild in a data-driven fashion.

    The configuration file is generated by comparing the newspaper issue IDs
    contained in the s3 canonical bucket against those in the s3 rebuilt
    bucket.
    """
    config = []

    if issues_to_rebuild is None:
        issues_to_ingest, issues_to_rebuild = check_sync_rebuilt(
            canonical_bucket_name,
            rebuilt_bucket_name
        )

    print((
        f'Watch out: there are {issues_to_rebuild.shape[0]} issues '
        f'that are already ingested ({canonical_bucket_name}) but not '
        f'yet rebuilt ({rebuilt_bucket_name}).'
    ))

    for key, group in issues_to_rebuild.groupby(by='newspaper_id'):
        missing_years = sorted(set(group.year))
        for year in missing_years:
            config.append(
                {
                    key: [year, year + 1]
                }
            )
    return config


def configure_ingestion(
    canonical_bucket_name: str,
    rebuilt_bucket_name: str,
    issues_to_ingest: pd.DataFrame = None
) -> list:
    """Generate the config file for data rebuild in a data-driven fashion.

    The configuration file is generated by comparing the newspaper issue IDs
    contained in the s3 canonical bucket against those in the s3 rebuilt
    bucket.
    """
    config = []

    if issues_to_ingest is None:
        issues_to_ingest, issues_to_rebuild = check_sync_rebuilt(
            canonical_bucket_name,
            rebuilt_bucket_name
        )

    print((
        f'Watch out: there are {issues_to_ingest.shape[0]} issues '
        f'that are already rebuilt ({rebuilt_bucket_name}) but not '
        f'yet ingested ({canonical_bucket_name}).'
    ))

    for key, group in issues_to_ingest.groupby(by='newspaper_id'):
        missing_years = sorted(set(group.year))
        config.append(
            {
                key: [min(missing_years), max(missing_years) + 1]
            }
        )
    return config


"""
All this eventually should be called with a CLI.
""" # noqa
